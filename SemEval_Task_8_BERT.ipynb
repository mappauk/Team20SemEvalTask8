{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLttzlDzrmy5"
      },
      "outputs": [],
      "source": [
        "# only run on google runtime\n",
        "!pip install tensorflow-text\n",
        "!pip install tf-models-official\n",
        "!pip install tensorflow-addons\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jDxRhOjosYHn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "from official.nlp import optimization\n",
        "import tensorflow_addons as tfa\n",
        "import transformers\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge7f2Wwksaea"
      },
      "outputs": [],
      "source": [
        "# only run on google runtime\n",
        "# update file paths with location to subtask data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "input_directory = '/content/drive/MyDrive/2023-2024 School Year/Fall Semester/Natural Language Processing/Project/Data'\n",
        "raw_train_data = input_directory + '/subtaskA_train_monolingual.jsonl'\n",
        "raw_dev_data = input_directory + '/subtaskA_dev_monolingual.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1x0oMpIXsd5_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def extract_data(filename):\n",
        "  text = []\n",
        "  labels = []\n",
        "  model = []\n",
        "  source = []\n",
        "  with open(filename, 'r', encoding='utf-8') as f:\n",
        "    jlist = list(f)\n",
        "    for elem in jlist:\n",
        "      jsonData = json.loads(elem)\n",
        "      text.append(jsonData[\"text\"])\n",
        "      labels.append(jsonData[\"label\"])\n",
        "      model.append(jsonData[\"model\"])\n",
        "      source.append(jsonData[\"source\"])\n",
        "  return text, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_3UvUEmjsgz8"
      },
      "outputs": [],
      "source": [
        "train_text, train_labels = extract_data(raw_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if using train set for eval\n",
        "train_text, eval_text, train_labels, eval_labels = train_test_split(train_text, train_labels, test_size=0.1, random_state=92)"
      ],
      "metadata": {
        "id": "22YDck6skPhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WnG0RolMsiV6"
      },
      "outputs": [],
      "source": [
        "# if using dev set for eval\n",
        "train_text, train_labels = sk.utils.shuffle(train_text, train_labels, random_state=92)\n",
        "eval_text, eval_labels = extract_data(raw_dev_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZLaK71bsjzi"
      },
      "outputs": [],
      "source": [
        "# if using deberta\n",
        "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "bert_encoder = transformers.TFDebertaModel.from_pretrained(\"microsoft/deberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91xndEzNskeS"
      },
      "outputs": [],
      "source": [
        "# if using bert\n",
        "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_encoder = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0VN_7ybAsmWl"
      },
      "outputs": [],
      "source": [
        "def hugging_face_bert_encode(text_data, encode_ending=False):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  for text in text_data:\n",
        "    if encode_ending:\n",
        "      text_split = text.split()\n",
        "      end_of_text = text_split[max(-1*len(text_split), -384):]\n",
        "      text = ' '.join(end_of_text)\n",
        "    tokenized_data = bert_tokenizer(text, padding='max_length', max_length=512, truncation=True)\n",
        "    input_ids.append(tokenized_data['input_ids'])\n",
        "    attention_masks.append(tokenized_data['attention_mask'])\n",
        "  return [np.array(input_ids), np.array(attention_masks)]\n",
        "encode_from_end = False\n",
        "hugging_face_train_text_data = hugging_face_bert_encode(train_text, encode_from_end)\n",
        "hugging_face_train_labels = np.array(train_labels)\n",
        "hugging_face_test_text_data = hugging_face_bert_encode(eval_text, encode_from_end)\n",
        "hugging_face_test_labels = np.array(eval_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rC70syhxspKH"
      },
      "outputs": [],
      "source": [
        "# text classification modeling was based in part upon tensorflows guide to text classification with BERT: https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "def build_model():\n",
        "  input_ids = tf.keras.Input(shape=(512,),dtype='int32')\n",
        "  attention_masks = tf.keras.Input(shape=(512,),dtype='int32')\n",
        "  bert_outputs = bert_encoder([input_ids, attention_masks])['last_hidden_state']\n",
        "  pooling_layer = tf.keras.layers.GlobalMaxPool1D()\n",
        "  dropout_inputs = pooling_layer(bert_outputs)\n",
        "  dropout = tf.keras.layers.Dropout(0.2)\n",
        "  classifier_inputs = dropout(dropout_inputs)\n",
        "  classifier = tf.keras.layers.Dense(1, activation='sigmoid', name='output')\n",
        "  outputs = classifier(classifier_inputs)\n",
        "  return tf.keras.Model([input_ids, attention_masks], outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2neuiWFisp22"
      },
      "outputs": [],
      "source": [
        "final_train_data = hugging_face_train_text_data\n",
        "final_train_labels = hugging_face_train_labels\n",
        "final_test_data = hugging_face_test_text_data\n",
        "final_test_labels = hugging_face_test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGtf1mcQsrnZ"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "#optimizer\n",
        "batch_size = 4\n",
        "epochs = 1\n",
        "steps_per_epoch = int(hugging_face_test_text_data[0].shape[0] /batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "#loss\n",
        "loss= tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "#metrics\n",
        "metrics=[\n",
        "    tfa.metrics.F1Score(num_classes=1, average=\"micro\", name='micro_f1', threshold=0.5),\n",
        "    tf.keras.metrics.BinaryAccuracy()\n",
        "]\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "model.fit(x=(final_train_data[0], final_train_data[1]), y=final_train_labels, validation_data = (final_test_data, final_test_labels), batch_size=batch_size, epochs=epochs)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}