{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GgBoeI-HlnG"
      },
      "outputs": [],
      "source": [
        "# only run on google runtime\n",
        "!pip install tensorflow-text\n",
        "!pip install tf-models-official\n",
        "!pip install tensorflow-addons\n",
        "!pip install scikit-learn\n",
        "!pip install transformers\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ13VvHEK4e6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "from official.nlp import optimization\n",
        "import tensorflow_addons as tfa\n",
        "import transformers\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GAxyYeJdhEXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3caaf1f9-7f07-46dd-f457-55372d39b801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# only run on google runtime\n",
        "# update paths to semeval task data on machine\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "input_directory = '/content/drive/MyDrive/2023-2024 School Year/Fall Semester/Natural Language Processing/Project/Data'\n",
        "raw_train_data = input_directory + '/subtaskA_train_monolingual.jsonl'\n",
        "raw_dev_data = input_directory + '/subtaskA_dev_monolingual.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tEAegwhb1vVn"
      },
      "outputs": [],
      "source": [
        "# update with path to location of saved numpy array files generated by probability encoder notebook\n",
        "train_encoded_probabilities = np.load(\"/content/drive/MyDrive/2023-2024 School Year/Fall Semester/Natural Language Processing/Project/train_final_encoded_probabilities.npy\")\n",
        "dev_encoded_probabilities = np.load(\"/content/drive/MyDrive/2023-2024 School Year/Fall Semester/Natural Language Processing/Project/dev_final_encoded_probabilities.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def extract_labels(filename):\n",
        "  labels = []\n",
        "  with open(filename, 'r', encoding='utf-8') as f:\n",
        "    jlist = list(f)\n",
        "    for elem in jlist:\n",
        "      jsonData = json.loads(elem)\n",
        "      labels.append(jsonData[\"label\"])\n",
        "  return labels"
      ],
      "metadata": {
        "id": "1W3y3JNApf30"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = extract_labels(raw_train_data)\n",
        "dev_labels = extract_labels(raw_dev_data)"
      ],
      "metadata": {
        "id": "KV667uW7plbR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if using train text for validation\n",
        "train_encoded_probabilities, eval_encoded_probabilities, train_labels, eval_labels = train_test_split(train_encoded_probabilities, train_labels, test_size=0.1, random_state=92)"
      ],
      "metadata": {
        "id": "1iXFWIc0VZzO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if using dev set for validation\n",
        "train_encoded_probabilities, train_labels = sk.utils.shuffle(train_encoded_probabilities, train_labels, random_state=92)\n",
        "eval_encoded_probabilities = dev_encoded_probabilities\n",
        "eval_labels = dev_labels"
      ],
      "metadata": {
        "id": "nCMnQE0sekRB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  inputs = tf.keras.Input(shape=(511, 2), dtype='float32')\n",
        "  lstm = tf.keras.layers.LSTM(units = 10, activation=\"tanh\", dropout=0.1)\n",
        "  bidirectional = tf.keras.layers.Bidirectional(lstm)\n",
        "  lstm_outputs = bidirectional(inputs)\n",
        "  classifier = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  outputs = classifier(lstm_outputs)\n",
        "  return tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "vl9iHCSs-iLs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uWUfNK3Js6sV"
      },
      "outputs": [],
      "source": [
        "final_train_data = train_encoded_probabilities\n",
        "final_train_labels = np.array(train_labels)\n",
        "final_test_data = eval_encoded_probabilities\n",
        "final_test_labels = np.array(eval_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='checkpointbest', save_best_only=True, save_weights_only=True, monitor=\"val_loss\", mode=\"min\")"
      ],
      "metadata": {
        "id": "AmLWr9hMSFJl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW8Yz23ILfAj"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "#optimizer\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "steps_per_epoch = int(final_train_data.shape[0] / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 0.001\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "#loss\n",
        "loss= tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "#metrics\n",
        "metrics=[\n",
        "    tfa.metrics.F1Score(num_classes=1, average=\"micro\", name='micro_f1', threshold=0.5),\n",
        "    tf.metrics.BinaryAccuracy()\n",
        "]\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "model.fit(x=final_train_data, y=final_train_labels, validation_data = (final_test_data, final_test_labels), batch_size=batch_size, epochs=epochs, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5GSu014cyec"
      },
      "outputs": [],
      "source": [
        "model.load_weights('checkpointbest')\n",
        "model.evaluate(x=final_test_data, y=final_test_labels, batch_size=batch_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}